{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Bird Book</font>\n",
    "## <font color='blue'>Chapter 1 : Language Processing and Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>1. Text and Words</font>\n",
    "#### 1.1 Searching Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Context of where the word occures\n",
    "text5.concordance(\"Lol\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# other words in the similar contexts\n",
    "text5.similar(\"Lol\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common contexts\n",
    "text5.common_contexts([\"Lol\",\"hi\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot of positional occurance of words \n",
    "text4.dispersion_plot([\"god\", \"citizens\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Counting Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove duplicate words (dictionary or type of words)\n",
    "set(text3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show sorted\n",
    "sorted(set(text3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text))/len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text5.count(\"Yo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>2. A closer look at Python</font>\n",
    "#### 2.1 Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex = [\"im\", \"here\", \"boy\"]\n",
    "# count specified element of list\n",
    "ex.count(\"im\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate two lists\n",
    "[\"hey\", \"you\"] + [\"are\", \"good\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex.append(\"nice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Indexing Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4[173]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first index of occurance of the word\n",
    "text4.index(\"god\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# slicing (in this case gets items from position -100 to last item)\n",
    "text5[-100:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify group of items\n",
    "ex[1:3] = [\"second\", \"third\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = \"Mahdi\"\n",
    "# index a string\n",
    "name[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# slice a string\n",
    "name[:4]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multiply a string\n",
    "name * 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate a string\n",
    "name + \"!\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "' '.join([\"hello\", \"world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"My name is Mahdi\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>3. Computing with language</font>\n",
    "#### 3.1 Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk's built-in frequency distribution\n",
    "fdist1 = FreqDist(text1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist1.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FreqDist format: [('Hello', 34),('Mahdi', 1344),('Ali', 123), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get frequency of word\n",
    "fdist1[\"shame\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cumulative frequency plot\n",
    "fdist1.plot(50, cumulative=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words that occures only once\n",
    "fdist1.hapaxes()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Fine-grained Selection of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list comprehension concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "long_words = [w for w in set(text4) if len(w) > 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "informative = [w for w in set(text5) if len(w) > 7 and FreqDist(text5)[w] > 7 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Collocations and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A collocation is a sequence of words that occur together unusually often like -> united states, persian gulf\n",
    "# Bigrams are just pair of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "list(bigrams(['more', 'is', 'said', 'than', 'done']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bigram format: [(\"my\", \"name\"), (\"name\", \"is\"), (\"is\", \"Mahdi\"), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text5.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Counting Other Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(len(w) for w in text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most frequent item\n",
    "fdist.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frequency of 3 in our FreqDist\n",
    "fdist.freq(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total number of samples\n",
    "fdist.N() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tabulate the frequency distribution\n",
    "fdist.tabulate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update fdist1 with counts from fdist2\n",
    "fdist1 |= fdist2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test if samples in fdist1 occur less frequently than in fdist2\n",
    "fdist1 < fdist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>4 Python decision and control</font>\n",
    "#### 4.1 Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word comparison operators\n",
    "s.startswith(t) # test if s starts with t\n",
    "s.endswith(t)   # test if s ends with t\n",
    "t in s          # test if t is a substring of s\n",
    "s.islower()     # test if s contains cased characters and all are lowercase\n",
    "s.isupper()     # test if s contains cased characters and all are uppercase\n",
    "s.isalpha()     # test if s is non-empty and all characters in s are alphabetic\n",
    "s.isalnum()     # test if s is non-empty and all characters in s are alphanumeric\n",
    "s.isdigit()     # test if s is non-empty and all characters in s are digits\n",
    "s.istitle()     # test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Chapter 2 : Accessing Text Corpora and Lexical Resources</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>1. Accessing Text Corpora</font>\n",
    "#### 1.1 Gutenberg Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 25,000 free electronic books\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # file ids of this corpus\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words of this fileid\n",
    "gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using prior tasks of chapter 1 on nltk's corpus\n",
    "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
    "emma.concordance(\"surprize\")\n",
    "emma.similar(\"surprize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw text of this fileid\n",
    "gutenberg.raw('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentences of this fileid\n",
    "gutenberg.sents('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileid = 'austen-emma.txt'\n",
    "num_chars = len(gutenberg.raw(fileid))\n",
    "num_words = len(gutenberg.words(fileid))\n",
    "num_sents = len(gutenberg.sents(fileid))\n",
    "num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "# average word length\n",
    "avg_word_len = num_chars / num_words\n",
    "# average sentence length\n",
    "avg_sent_len = num_words / num_sents\n",
    "lexical_diversity = num_words / num_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Web and Chat Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Firefox discussion forum, conversations overheard in New York,\n",
    "the movie script of Pirates of the Carribean, personal advertisements, and wine reviews\n",
    "\"\"\"\n",
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instant messaging chat sessions\n",
    "from nltk.corpus import nps_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chatroom of 20's which contain 706 posts collected on 10-19-2006\n",
    "nps_chat.posts(\"10-19-20s_706posts.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this corpus contains categorized text\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get categories\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get words by category\n",
    "brown.words(categories='news')\n",
    "# get words by fileid\n",
    "brown.words(fileids=['cg22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get sentences by category\n",
    "brown.sents(categories=['news', 'editorial', 'reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count wh modals within each category\n",
    "from nltk import ConditionalFreqDist\n",
    "cfd = ConditionalFreqDist(\n",
    "           (genre, word)\n",
    "           for genre in brown.categories()\n",
    "           for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Reuters Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "10,788 news documents totaling 1.3 million words. \n",
    "The documents have been classified into 90 topics, and grouped into two sets, called \"training\" and \"test\"\n",
    "Unlike the Brown Corpus, categories in the Reuters corpus overlap with each other, \n",
    "simply because a news story often covers multiple topics\n",
    "\"\"\"\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get fileid\n",
    "reuters.fileids()\n",
    "reuters.fileids('barley')\n",
    "reuters.fileids(['barley', 'corn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get category\n",
    "reuters.categories()\n",
    "reuters.categories('training/9865')\n",
    "reuters.categories(['training/9865', 'training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get words\n",
    "reuters.words('training/9865')[:14]\n",
    "reuters.words(['training/9865', 'training/9880'])\n",
    "reuters.words(categories='barley')\n",
    "reuters.words(categories=['barley', 'corn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Inaugural Address Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the corpus is actually a collection of 55 texts, one for each presidential address.\n",
    "An interesting property of this collection is its time dimension\n",
    "\"\"\"\n",
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Corpora in Other Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spanish corpus\n",
    "from nltk.corpus import cess_esp\n",
    "# indian cropus\n",
    "from nltk.corpus import indian\n",
    "# Universal Declaration of Human Rights\n",
    "from nltk.corpus import udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uhdr.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udhr.words('Farsi_Persian-UTF8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 Text Corpus Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic corpus functionality\n",
    "fileids()  # the files of the corpus\n",
    "fileids([categories])  # the files of the corpus corresponding to these categories\n",
    "categories()  # the categories of the corpus\n",
    "categories([fileids])  # the categories of the corpus corresponding to these files\n",
    "raw()  # the raw content of the corpus\n",
    "raw(fileids=[f1,f2,f3])  # the raw content of the specified files\n",
    "raw(categories=[c1,c2])  # the raw content of the specified categories\n",
    "words()  # the words of the whole corpus\n",
    "words(fileids=[f1,f2,f3])  # the words of the specified fileids\n",
    "words(categories=[c1,c2])  # the words of the specified categories\n",
    "sents()  # the sentences of the whole corpus\n",
    "sents(fileids=[f1,f2,f3])  # the sentences of the specified fileids\n",
    "sents(categories=[c1,c2])  # the sentences of the specified categories\n",
    "abspath(fileid)  # the location of the given file on disk\n",
    "encoding(fileid)  # the encoding of the file (if known)\n",
    "open(fileid)  # open a stream for reading the given corpus file\n",
    "root  # if the path to the root of locally installed corpus\n",
    "readme()  # the contents of the README file of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 Loading your own Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_root = \"C:\\\\Users\\\\Mahdi\\\\Desktop\"\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '[\\w]*\\.txt')\n",
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>2. Conditional Frequency Distributions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A conditional frequency distribution is a collection of frequency distributions, each one for a different \"condition\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Conditions and Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to process pair of events in Conditional Frequency Distribution\n",
    "[('news', 'The'), ('news', 'Fulton'), ('romance', 'County'), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each pair has the form (condition, event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Counting Words by Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all conditions\n",
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get specified condition\n",
    "cfd['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most common in conditional frequency distribution\n",
    "cfd['romance'].most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get specified word of specified condition\n",
    "cfd['romance']['could']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Plotting and Tabulating Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.tabulate(conditions=[], samples=[], cumulative=True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Generating Random Text with Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model(cfdist, word, num=15):\n",
    "    for i in range(num):\n",
    "        print(word, end=' ')\n",
    "        word = cfdist[word].max()\n",
    "\n",
    "text = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "bigrams = nltk.bigrams(text)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_model(cfd, 'living')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Conditional Frequency Distribution methods\n",
    "cfdist = ConditionalFreqDist(pairs)  # create a conditional frequency distribution from a list of pairs\n",
    "cfdist.conditions()  # the conditions\n",
    "cfdist[condition]  # the frequency distribution for this condition\n",
    "cfdist[condition][sample]  # frequency for the given sample for this condition\n",
    "cfdist.tabulate()  # tabulate the conditional frequency distribution\n",
    "cfdist.tabulate(samples, conditions)  # tabulation limited to the specified samples and conditions\n",
    "cfdist.plot()  # graphical plot of the conditional frequency distribution\n",
    "cfdist.plot(samples, conditions)  # graphical plot limited to the specified samples and conditions\n",
    "cfdist1 < cfdist2  # test if samples in cfdist1 occur less frequently than in cfdist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>4. Lexical Resources</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A lexicon, or lexical resource, is a collection of words and/or phrases\n",
    "along with associated information such as part of speech and sense definitions\n",
    "A lexical entry consists of a headword (also known as a lemma) \n",
    "along with additional information such as the part of speech and the sense definition\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of lexicon : FreqDist, Dictionary, cfd, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Two distinct words having the same spelling are called homonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Wordlist Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Words Corpus is used by some spell checkers\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "stopwords, that is, high-frequency words like the, to and also\n",
    "that we sometimes want to filter out of a document before further processing\n",
    "\"\"\"\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unusual words are those which are not in dictionary\n",
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "    eng_vocab = set(w.lower() for w in words.words())\n",
    "    unusual = text_vocab - eng_vocab\n",
    "    return sorted(unusual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8,000 first names categorized by gender\n",
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Pronouncing Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CMU Pronouncing Dictionary for US English, which was designed for use by speech synthesizers\n",
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format of output [('word',['PH1', 'PH2', 'PH3', ...]), (), (), ...]\n",
    "cmudict.entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary of words\n",
    "cmudict.dict()\n",
    "# text-to-speech\n",
    "cmudict.dict()['fire']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Comparative Wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NLTK includes so-called Swadesh wordlists, lists of about 200 common words in several languages.\n",
    "The languages are identified using an ISO 639 two-letter code\n",
    "\"\"\"\n",
    "from nltk.corpus import swadesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get file ids (output two-letter ISO code for each lang)\n",
    "swadesh.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get english words in the corpus\n",
    "swadesh.words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# French to English translator\n",
    "fr2en = swadesh.entries(['fr', 'en'])\n",
    "translate = dict(fr2en)\n",
    "translate['chien']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Shoebox and Toolbox Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Toolbox file consists of a collection of entries, where each entry is made up of one or more fields. \n",
    "Most fields are optional or repeatable, \n",
    "which means that this kind of lexical resource cannot be treated as a table or spreadsheet\n",
    "\"\"\"\n",
    "from nltk.corpus import toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a dictionary for the Rotokas language\n",
    "toolbox.entries('rotokas.dic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>5. WordNet</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WordNet is a semantically-oriented dictionary of English,\n",
    "similar to a traditional thesaurus but with a richer structure\n",
    "\"\"\" \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Senses and Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get synonym set of a word as Synset object\n",
    "wordnet.synsets('automobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a synset with specific POS\n",
    "wordnet.synsets('good', wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a collection of synonomous words of this synset called lemma\n",
    "wordnet.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the lemmas for a given synset\n",
    "wordnet.synset('car.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up a particular lemma\n",
    "wordnet.lemma('car.n.01.automobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the synset corresponding to a lemma\n",
    "wordnet.lemma('car.n.01.automobile').synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the name of a lemma\n",
    "wordnet.lemma('car.n.01.automobile').name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the lemmas of specified word\n",
    "wordnet.lemmas('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 The WordNet Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get child nodes in synset hierarchy\n",
    "wordnet.synset('car.n.01').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get parent nodes in synset hierarchy\n",
    "wordnet.synset('car.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get hypernym paths to the root of tree\n",
    "wordnet.synset('car.n.01').hypernym_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get root hypernyms of specified synset\n",
    "wordnet.synset('car.n.01').root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 More Lexical Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Another important way to navigate the WordNet network is from items to their components (meronyms) \n",
    "or to the things they are contained in (holonyms)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the part's(organ) of a word which contains -> in this case 'crown', 'limb', ...\n",
    "wordnet.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the substance of a word which made of it -> in this case 'heartwood', 'sapwood'\n",
    "wordnet.synset('tree.n.01').substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the word is member of what words -> in this case earth is member of  'solar_system'\n",
    "wordnet.synset('earth.n.01').member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# verbs which are the consequences of this action -> in this case walk consists of 'step'\n",
    "wordnet.synset('walk.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# contrast of a lemma\n",
    "wordnet.lemma('supply.n.02.supply').antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get lowest common hypernyms in hierarchy of two synsets\n",
    "cat = wordnet.synset('cat.n.01')\n",
    "dog = wordnet.synset('dog.n.01')\n",
    "cat.lowest_common_hypernyms(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get min depth of synset in hierarchy of synsets\n",
    "wordnet.synset('cat.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "path_similarity assigns a score in the range 0–1 based on the shortest path\n",
    "that connects the concepts in the hypernym hierarchy \n",
    "(-1 is returned in those cases where a path cannot be found)\n",
    "\"\"\"\n",
    "engineer = wordnet.synset('engineer.n.01')\n",
    "architect = wordnet.synset('architect.n.01')\n",
    "architect.path_similarity(engineer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Jacob Book</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Chapter 1 : Tokenizing Text and WordNet Basics</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.2 Tokenizing text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This tokenizer divides a text into a list of sentences, \n",
    "by using an unsupervised algorithm to build a model for abbreviation words\n",
    ", collocations, and words that start sentences\n",
    "\"\"\"\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sent tokenizer\n",
    "sent_tokenize('Hi students! Welcome to our course.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a PunktSentenceTokenizer instance to reuse our tokenizer\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers\\punkt\\english.pickle')\n",
    "tokenizer.tokenize('Hi students! Welcome to our course.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 Tokenizing Farsi text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# farsi sent tokenizer\n",
    "from hazm import sent_tokenize\n",
    "para = 'سلام. این متن، جهت تست نوشته شده است'\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Tokenizing sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# version 1 word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('Hello world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# verion 2 word tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize('Hello world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# version 3 word tokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# farsi word tokenizer\n",
    "from hazm import word_tokenize\n",
    "word_tokenize('این جمله، برای تست نوشته شده است.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Tokenizing sentences using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# version 1 regex tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# version 2 regex tokenizer (without instansiation)\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(\"can't is a contraction.\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regex tokenizer split's on gaps\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps = True)\n",
    "tokenizer.tokenize(\"can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Training a sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using webtext corpus\n",
    "# PunktSentenceTokenizer uses an unsupervized ML algorithm\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "sent_tokenizer.tokenize(\"Hello! aren't you ready for battle?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading text version 2\n",
    "with open('C:\\\\Users\\\\Mahdi\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\webtext\\\\overheard.txt', encoding='ISO-8859-2') as f:\n",
    "    text = f.read()\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Filtering stopwords in a tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(\"Here is a good city\")\n",
    "[w for w in words if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Looking up Synsets for a word in WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get POS of a synset\n",
    "wordnet.synset('book.n.01').pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get synset with specified POS\n",
    "wordnet.synsets('great')\n",
    "wordnet.synsets('great', pos = 'n')\n",
    "wordnet.synsets('great', pos = 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 Looking up lemmas and synonyms in WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 Calculating WordNet Synset similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# version 1 similarity measure\n",
    "dog = wordnet.synset('dog.n.01')\n",
    "cat = wordnet.synset('cat.n.01')\n",
    "cat.wup_similarity(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shortest path distance between two synsets\n",
    "cat.shortest_path_distance(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# version 2 similarity measure\n",
    "cat.lch_similarity(dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10 Discovering word collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BigramCollocationFinder\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "words = [w.lower() for w in webtext.words('grail.txt')]\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)  # get top 4 collocations of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first 4 bigram collocation along with their score as a tuple of (collocation, score)\n",
    "bcf.score_ngrams(BigramAssocMeasures.likelihood_ratio)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get bigram collocations above specific score\n",
    "list(bcf.above_score(BigramAssocMeasures.likelihood_ratio, 220))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter stopwords in BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset  # this will filter words which are stopwords or smaller than 3\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)  # get top 4 collocations of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "words = [w.lower() for w in webtext.words('singles.txt')]\n",
    "tcf = TrigramCollocationFinder.from_words(words)\n",
    "tcf.apply_word_filter(filter_stops)  # fiter stopwords\n",
    "tcf.apply_freq_filter(3)  # filter less frequent words\n",
    "tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 4)  # get top 4 collocations of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Chapter 2 : Replacing and Correcting Words</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Stemming words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stemming is to remove affixes from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# porter stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('cookery')  # output 'cookeri'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lancaster stemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('cookery')  # output 'cookery'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# farsi stemmer\n",
    "from hazm import Stemmer\n",
    "stemmer = Stemmer()\n",
    "stemmer.stem('میرویم')  # output 'میرو'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regex stemmer that removes any prefix or suffix that matches the expression\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmer = RegexpStemmer('ing')\n",
    "stemmer.stem('booking')  # output 'book'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# snowball stemmer contains 13 languages along with 2 english stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "stemmer.stem('mahalo')  # output 'mahal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# languages used in snowball stemmer (class variable)\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Lemmatizing words with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lemma is root of a word\n",
    "unlike stem it's a valid word\n",
    "look at the meaning\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordnet lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('cooking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default pos for lemmatize method is pos=NOUN\n",
    "lemmatizer.lemmatize('cooking', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# farsi lemmatizer\n",
    "from hazm import Lemmatizer\n",
    "lemmatizer = Lemmatizer()\n",
    "lemmatizer.lemmatize('می‌روم')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Replacing words matching regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regexp replacer\n",
    "import re\n",
    "replacement_patterns = [\n",
    "     (r'won\\'t', 'will not'),\n",
    "     (r'can\\'t', 'cannot'),\n",
    "     (r'([i,I])\\'m', '\\g<1> am'),\n",
    "     (r'ain\\'t', 'is not'),\n",
    "     (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "     (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "     (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "     (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "     (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "     (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regexp replacer usage\n",
    "replacer = RegexpReplacer(replacement_patterns)\n",
    "replacer.replace(\"i'm not gonna do that, you can't be serious, i'd rather not to go there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repeat replacer\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "class RepeatReplacer:\n",
    "  def __init__(self):\n",
    "    self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    self.repl = r'\\1\\2\\3'\n",
    "  def replace(self, word):\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "    if repl_word != word:\n",
    "      return self.replace(repl_word)\n",
    "    else:\n",
    "      return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repeat replacer usage\n",
    "repeat_replacer = RepeatReplacer()\n",
    "repeat_replacer.replace('goooood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Spelling correction with Enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spell checker\n",
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "class SpellingReplacer(object):\n",
    "    def __init__(self, dict_name='en', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        else:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spell checker usage\n",
    "spell_checker = SpellingReplacer()\n",
    "spell_checker.replace('cookbok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enchant list languages\n",
    "enchant.list_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enchant check if language exist\n",
    "enchant.dict_exists('en_GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enchant check word correctness\n",
    "import enchant\n",
    "enchant.Dict('en_US').check('theater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom spell checker\n",
    "class CustomSpellingReplacer(SpellingReplacer):\n",
    "    def __init__(self, spell_dict, max_dist=2):\n",
    "        self.spell_dict = spell_dict\n",
    "        self.max_dist = max_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom spell checker with personal word list\n",
    "d = enchant.DictWithPWL('en_US', 'my_words.txt')\n",
    "replacer = CustomSpellingReplacer(d)\n",
    "replacer.replace('bok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Replacing Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base class for word replacer -> this class replace a word by it's synonym from our dictionary\n",
    "class WordReplacer(object):\n",
    "    def __init__(self, word_map):\n",
    "        self.word_map = word_map\n",
    "    def replace(self, word):\n",
    "        return self.word_map.get(word, word)  # dict.get(key, default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base word replacer example usage\n",
    "word_replacer = WordReplacer({'bday': 'birthday'})\n",
    "word_replacer.replace('bday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word replacer from csv file\n",
    "import csv\n",
    "\n",
    "class CsvWordReplacer(WordReplacer):\n",
    "    def __init__(self, fname):\n",
    "        word_map = {}\n",
    "        for line in csv.reader(open(fname)):\n",
    "            word, syn = line\n",
    "            word_map[word] = syn\n",
    "        super().__init__(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# csv word replacer example usage\n",
    "replacer = CsvWordReplacer('csv_test.csv')\n",
    "replacer.replace('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word replacer from yaml file\n",
    "import yaml\n",
    "\n",
    "class YamlWordReplacer(WordReplacer):\n",
    "    def __init__(self, fname):\n",
    "        word_map = yaml.load(open(fname))\n",
    "        super().__init__(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yaml word replacer example usage\n",
    "replacer = YamlWordReplacer('yaml_test.yaml')\n",
    "replacer.replace('bday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Replacing negations with antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# antonym replacer -> replace negative words with positive ones\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class AntonymReplacer(object):\n",
    "    def replace(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "        for syn in wordnet.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "        if len(antonyms) == 1: # unambiguous replacement\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def replace_negations(self, orig_sent):\n",
    "        sent = word_tokenize(orig_sent)\n",
    "        i, l = 0, len(sent)\n",
    "        words = []\n",
    "        while i < l:\n",
    "            word = sent[i]\n",
    "            if word == 'not' and i+1 < l:\n",
    "                ant = self.replace(sent[i+1])\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Antonym replacer example usage\n",
    "replacer = AntonymReplacer()\n",
    "replacer.replace_negations(\"Let's not uglify our code!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inherit __init__ and replace from WordReplacer and replace_negations from AntonymReplacer\n",
    "class AntonymWordReplacer(WordReplacer, AntonymReplacer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AntonymWordReplacer example usage\n",
    "replacer = AntonymWordReplacer({'ruin':'build'})\n",
    "replacer.replace_negations(\"Let's not ruin our code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Chapter 4 : Part-of-speech Tagging</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Default tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for every word will give default specified tag in constructor\n",
    "from nltk.tag import DefaultTagger\n",
    "tagger = DefaultTagger('NN')\n",
    "tagger.tag(['This', 'is', 'just', 'for', 'test'])  # output -> [('This', 'NN'), ('is', 'NN'), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# treebank corpus which is a tagged corpus\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get treebank sents\n",
    "treebank.sents()  # output -> [['w', 'w', 'w', ...], [], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get treebank tagged sents\n",
    "treebank.tagged_sents()  # output -> [[('w', 'tag'), ('w', 'tag'), ...], [], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate default tagger with treebank tagged corpus\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "tagger.evaluate(test_sents)  # output -> 0.143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tagging sentences with default tagger\n",
    "tagger.tag_sents([['Hi', 'students'], ['How', 'are', 'you']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# untagging a tagged sentence\n",
    "from nltk.tag import untag\n",
    "untag([('Hello', 'NN'), ('students', 'NN')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Training a unigram part-of-speech tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unigram tagger \n",
    "from nltk.tag import UnigramTagger\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "tagger = UnigramTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tag with unigram tagger\n",
    "tagger.tag(treebank.sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate unigram tagger\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "tagger.evaluate(test_sents)  # output -> 0.858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# override context model\n",
    "tagger = UnigramTagger(model={'Pierre': 'NN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set minimum frequency cutoff for unigram tagger\n",
    "tagger = UnigramTagger(train_sents, cutoff=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Combining taggers with backoff tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# backoff will be used for those words which don't get any tag with one tagger\n",
    "# in this case untagged words goes to next tagger method\n",
    "tagger1 = DefaultTagger('NN')\n",
    "tagger2 = UnigramTagger(train_sents, backoff=tagger1)\n",
    "tagger2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get taggers in used by this object\n",
    "tagger1._taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if taggers in use are equal to specified taggers\n",
    "tagger2._taggers == [tagger2, tagger1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save and load a trained tagger as pickle file\n",
    "import pickle\n",
    "f = open('tagger.pickle', 'wb')\n",
    "pickle.dump(tagger, f)  # save model\n",
    "f.close()\n",
    "f = open('tagger.pickle', 'rb')\n",
    "tagger = pickle.load(f)  # load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Training and combining ngram taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NgramTagger childs -> UnigramTagger, BigramTagger, TrigramTagger\n",
    "NgramTagger parent tree -> ContextTagger -> SequentialBackoffTagger\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BigramTagger usage\n",
    "bitagger = BigramTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TrigramTagger usage\n",
    "tritagger = TrigramTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chain taggers together one after another with this backoff tagger method\n",
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff  # last class of list will be instantiated and returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chain taggers together\n",
    "from nltk.tag import DefaultTagger\n",
    "backoff = DefaultTagger('NN')\n",
    "# DefaultTagger -> backoff of UnigramTagger, UnigramTagger -> backoff of BigramTagger, ... \n",
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=backoff)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get context tags of BigramTagger\n",
    "bitagger._context_to_tag  # output -> {((previous_tag,), 'word'):'tag', ():'', ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# QuadgramTagger\n",
    "from nltk.tag import NgramTagger\n",
    "\n",
    "quadtagger = NgramTagger(4, train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# QuadgramTagger class to enable us to use it as backoff tagger\n",
    "class QuadgramTagger(NgramTagger):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        NgramTagger.__init__(self, 4, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# QuadgramTagger usage in backoff_tagger\n",
    "quadtagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger, QuadgramTagger], backoff=backoff)\n",
    "quadtagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note : too much context can have a negative effect on accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 Creating a model of likely word tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model for most common words\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "def word_tag_model(words, tagged_words, limit=1000):\n",
    "    fd = FreqDist(words)\n",
    "    cfd = ConditionalFreqDist(tagged_words)\n",
    "    most_freq = (word for word, count in fd.most_common(limit))\n",
    "    return dict((word, cfd[word].max()) for word in most_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# likely tagger usage\n",
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger, DefaultTagger\n",
    "\n",
    "model = word_tag_model(treebank.words(), treebank.tagged_words())\n",
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=DefaultTagger('NN'))\n",
    "likely_tagger = UnigramTagger(model=model, backoff=tagger)  # use custom model of most common words\n",
    "likely_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7 Tagging with regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regexp tagger\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "patterns = [\n",
    "     (r'^\\d+$', 'CD'),  # cardinal number , ex. 123\n",
    "     (r'.*ing$', 'VBG'), # gerunds, ex. wondering\n",
    "     (r'.*ment$', 'NN'), # noun, ex. wonderment\n",
    "     (r'.*ful$', 'JJ'), # adjective, ex. wonderful\n",
    "     (r'.*', 'NN')  # this statement will reach if previous ones don't match , act's like DefaultTagger\n",
    "]\n",
    "tagger = RegexpTagger(patterns)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regexp tagger is subclass of SequentialBackoffTagger, it means we can chain it to another tagger\n",
    "from nltk.tag import RegexpTagger, UnigramTagger, BigramTagger, TrigramTagger, DefaultTagger\n",
    "\n",
    "regexp_tagger = RegexpTagger(patterns, backoff=DefaultTagger('NN'))\n",
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=regexp_tagger)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8 Afix tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AfixTagger is subclass of ContextTagger\n",
    "the context is either the prefix or the suffix of a word\n",
    "AffixTagger class is able to learn tags based on fixed-length substrings of the beginning or ending of a word\n",
    "words must be at least five characters long. If a word is less than five characters, then None is returned as the tag\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with default arguments\n",
    "from nltk.tag import AffixTagger\n",
    "tagger = AffixTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix_tagger = AffixTagger(train_sents, affix_length = 3)  # three character prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suffix_tagger = AffixTagger(train_sents, affix_length = -2)  # two character suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To view the dictionary that maps contexts to tag\n",
    "tagger._context_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine AffixTagger as backoff chain\n",
    "pre3 = AffixTagger(train_sents, affix_length=3)\n",
    "pre2 = AffixTagger(train_sents, affix_length=2, backoff=pre3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this acts like unigram tagger (all the word is prefix)\n",
    "# if len of word is less than (min_stem_length + affix_length) then None is returned\n",
    "tagger = AffixTagger(train_sents, min_stem_length=0, affix_length=0)  # default value of min_stem_length = 2\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.9 Training a Brill tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "brill tagger is not subclass of SequentialBackoffTagger\n",
    "it uses some rules to correct the result of initial tagger\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# brill tagger\n",
    "from nltk.tag import brill, brill_trainer # two modules\n",
    "\n",
    "def train_brill_tagger(initial_tagger, train_sents, **kwargs):\n",
    "    templates = [\n",
    "       brill.Template(brill.Pos([-1])),\n",
    "       brill.Template(brill.Pos([1])),\n",
    "       brill.Template(brill.Pos([-2])),\n",
    "       brill.Template(brill.Pos([2])),\n",
    "       brill.Template(brill.Pos([-2, -1])),\n",
    "       brill.Template(brill.Pos([1, 2])),\n",
    "       brill.Template(brill.Pos([-3, -2, -1])),\n",
    "       brill.Template(brill.Pos([1, 2, 3])),\n",
    "       brill.Template(brill.Pos([-1]), brill.Pos([1])),\n",
    "       brill.Template(brill.Word([-1])),\n",
    "       brill.Template(brill.Word([1])),\n",
    "       brill.Template(brill.Word([-2])),\n",
    "       brill.Template(brill.Word([2])),\n",
    "       brill.Template(brill.Word([-2, -1])),\n",
    "       brill.Template(brill.Word([1, 2])),\n",
    "       brill.Template(brill.Word([-3, -2, -1])),\n",
    "       brill.Template(brill.Word([1, 2, 3])),\n",
    "       brill.Template(brill.Word([-1]), brill.Word([1]))\n",
    "    ]\n",
    "    trainer = brill_trainer.BrillTaggerTrainer(initial_tagger, templates, deterministic=True)\n",
    "    return trainer.train(train_sents, **kwargs) # returns an instance of the BrillTagger class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# brill tagger usage\n",
    "default_tagger = DefaultTagger('NN')\n",
    "initial_tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=default_tagger)\n",
    "brill_tagger = train_brill_tagger(initial_tagger, train_sents)\n",
    "brill_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.10 Training the TnT tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TnT stands for Trigrams'n'Tags. It is a statistical tagger based on second order Markov models.\n",
    "based on probability\n",
    "uses all the ngram models together to choose the best tag\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import tnt\n",
    "tnt_tagger = tnt.TnT()  # kwarg C is for make Capitalization significant, default is C=False (don't your capitalization)\n",
    "tnt_tagger.train(train_sents) # different from the previous tagger, you should explicitly call train() method\n",
    "tnt_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for tag unknown words you should use unseen tagger like -> DefaultTagger, AffixTagger, RegexpTagger\n",
    "unk = DefaultTagger('NN')  # tagger for unknown words\n",
    "tnt_tagger = tnt.TnT(unk=unk, Trained=True)  # Trained=True means that already trained\n",
    "tnt_tagger.train(train_sents)\n",
    "tnt_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.11 Using WordNet for tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WordNet can be useful for tagging unknown words\n",
    "mapping of wordnet tags to treebank tags\n",
    "v -> VB\n",
    "n -> NN\n",
    "a -> JJ\n",
    "s -> JJ\n",
    "r -> RB\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordnet tagger\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import SequentialBackoffTagger\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "class WordNetTagger(SequentialBackoffTagger):\n",
    "    '''\n",
    "     >>> wt = WordNetTagger()\n",
    "     >>> wt.tag(['food', 'is', 'great'])\n",
    "     [('food', 'NN'), ('is', 'VB'), ('great', 'JJ')]\n",
    "     '''\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        SequentialBackoffTagger.__init__(self, *args, **kwargs)\n",
    "        self.wordnet_tag_map = {\n",
    "            'n': 'NN', \n",
    "            's': 'JJ', \n",
    "            'a': 'JJ', \n",
    "            'r': 'RB', \n",
    "            'v': 'VB' \n",
    "        }\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        word = tokens[index]\n",
    "        fd = FreqDist()\n",
    "        for synset in wordnet.synsets(word):\n",
    "            fd[synset.pos()] += 1\n",
    "        if len(fd)==0:\n",
    "            return 'NN'\n",
    "        return self.wordnet_tag_map.get(fd.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordnet tagger example usage\n",
    "wnt = WordNetTagger()\n",
    "wnt.tag(['food', 'is', 'great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use wordnet in backoff chain\n",
    "tagger = backoff_tagger(train_sents, tagger_classes=[UnigramTagger, BigramTagger, TrigramTagger], backoff=wnt)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.12 Tagging proper names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this class could be chained\n",
    "from nltk.corpus import names\n",
    "from nltk.tag import SequentialBackoffTagger\n",
    "\n",
    "class NamesTagger(SequentialBackoffTagger):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        SequentialBackoffTagger.__init__(self, *args, **kwargs)\n",
    "        self.name_set = set([w.lower() for w in names.words()])\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        if tokens[index].lower() in self.name_set:\n",
    "            return 'NNP' # proper Noun\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NamesTagger example usage\n",
    "nt = NamesTagger()\n",
    "nt.tag(['James', 'book'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.13 Classifier-based tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this class implements a feature detector\n",
    "The feature detector finds multiple length suffixes, does some regular expression matching, \n",
    "and looks at the unigram, bigram, and trigram history to produce a fairly complete set of features for each word\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "tagger = ClassifierBasedPOSTagger(train=train_sents) # the first argument of the constructor is not 'train'\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: this is really slow\n",
    "# use custom classifier\n",
    "from nltk.classify import MaxentClassifier\n",
    "me_tagger = ClassifierBasedPOSTagger(train=train_sents, classifier_builder=MaxentClassifier.train)  # NaiveBayes is default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom feature detector\n",
    "from nltk.tag.sequential import ClassifierBasedTagger\n",
    "\n",
    "def unigram_feature_detector(tokens, index, history):\n",
    "    return {'word':tokens[index]} # should return a dictionary of feature-name:feature-value\n",
    "\n",
    "tagger = ClassifierBasedTagger(train=train_sents, feature_detector=unigram_feature_detector)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier tagger tags everything, until you use cutoff_prob and chain a backoff \n",
    "default = DefaultTagger('NN')\n",
    "tagger = ClassifierBasedPOSTagger(train=train_sents, cutoff_prob=0.3, backoff=default)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ClassifierBasedPOSTagger constructor has a parameter \"classifier\" which can be used for pretrained classifier\n",
    "# in this case \"classifier_builder\" will be ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.14 Training a tagger with NLTK-Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "it's impossible to know which methods and parameters will work best without doing training experiments. \n",
    "But training experiments can be tedious, since they often involve many small code changes (and lots of cut and paste). \n",
    "The author of this book (Jacob Perkins) created a project called NLTK-Trainer.\n",
    "The project is available on GitHub at https://github.com/japerk/nltk-trainer.\n",
    "In the terminal, go to the cloned folder and write the following command:\n",
    "python train_tagger.py treebank\n",
    "Look closely at the second line of output: 3914 tagged sents, training on 3914. \n",
    "This is a very misleading way to evaluate any trained model. \n",
    "To train on %75 of the corpus and test on other %25, write the following command (also to skip dumping a pickle file):\n",
    "python train_tagger.py treebank --fraction 0.75 --no-pickle\n",
    "The first argument to the script is corpus. \n",
    "This could be the name of an NLTK corpus that can be found in the nltk.corpus module, such as treebank or brown. \n",
    "It could also be the path to a custom corpus directory. \n",
    "If it's a path to a custom corpus, then you'll also need to use the --reader argument to specify the corpus reader class,\n",
    "such as nltk.corpus.reader.tagged.TaggedCorpusReader.\n",
    "The default training algorithm is aubt, which is shorthand for a sequential backoff tagger composed of \n",
    "AffixTagger + UnigramTagger + BigramTagger + TrigramTagger.\n",
    "You can determine other training algorithms and other options. Why not test it yourself?! Read pages 116-121\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Chapter 7 : Text Classification</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A binary classifier decides between two labels, such as spam detection. The text can either be one label or another, but not both.\n",
    "A multi-class classifier decides between three or more labels, such as topic detection.\n",
    "A multi-label classifier can assign one or more labels to a piece of text.\n",
    "classifier input corpus format should be -> [(featureset, label)] which featureset is a dict and label is it's class\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.1 Bag of words feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "in this chapter we want to classify a text, so we need feature set of a text\n",
    "a Bag of words is simply a dictionary of words presence which it's key is word and value is True\n",
    "format of Bag of words -> {'word1': True, 'word2': True, 'word3':True, ...}\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bag of words method\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude bad words from bag of words\n",
    "def bag_of_words_not_in_set(words, badwords):\n",
    "    return bag_of_words(set(words) - set(badwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include good words with bag of words\n",
    "def bag_of_words_in_set(words, goodwords):\n",
    "    return bag_of_words(set(words) & set(goodwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude stop words from bag of words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def bag_of_non_stopwords(words, stopfile='english'):\n",
    "    badwords = stopwords.words(stopfile)\n",
    "    return bag_of_words_not_in_set(words, badwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bag of words along with most significant bigrams of the text\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return bag_of_words(words + bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Training a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentiment analysis of movie_reviews corpus\n",
    "# movie_reviews corpus has two label -> 'pos', 'neg'\n",
    "from nltk.corpus import movie_reviews\n",
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# label feats from corpus\n",
    "# point is to extract features from text and turn them to an input format of Classification algorithm\n",
    "# output format -> {label: [featureset]}\n",
    "from collections import defaultdict\n",
    "   \n",
    "def label_feats_from_corpus(corp, feature_detector=bag_of_words):\n",
    "    label_feats = defaultdict(list)\n",
    "    for label in corp.categories():\n",
    "        for fileid in corp.fileids(categories=[label]):\n",
    "            feats = feature_detector(corp.words(fileids=[fileid]))\n",
    "            label_feats[label].append(feats)\n",
    "    return label_feats  # output format -> {'pos':[{bag_of_words}, {bag_of_words}, {}, ...], 'neg':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defaultdict is a dict which will return an empty value where you want to access to nonexist keys\n",
    "# defaultdict example 1\n",
    "s = 'mississippi'\n",
    "d = defaultdict(int)\n",
    "for k in s:\n",
    "    d[k] += 1\n",
    "d.items()\n",
    "# d['m'] -> 1\n",
    "# d['q'] -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defaultdict example 2\n",
    "s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]\n",
    "d = defaultdict(list)\n",
    "for k, v in s:\n",
    "    d[k].append(v)\n",
    "d.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create two train and test lists by spliting labelfeats\n",
    "def split_label_feats(lfeats, split=0.75):\n",
    "    train_feats = []\n",
    "    test_feats = []\n",
    "    for label, feats in lfeats.items():\n",
    "        cutoff = int(len(feats) * split)\n",
    "        train_feats.extend([(feat, label) for feat in feats[:cutoff]]) # it is better to first shuffle data\n",
    "        test_feats.extend([(feat, label) for feat in feats[cutoff:]])\n",
    "    return train_feats, test_feats  # output format -> [({bag of words}, 'pos/neg'), (), (), (),  ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split label feats example usage\n",
    "from nltk.corpus import movie_reviews\n",
    "lfeats = label_feats_from_corpus(movie_reviews)\n",
    "\n",
    "train_feats, test_feats = split_label_feats(lfeats)\n",
    "print(\"%d %d\" %(len(train_feats),len(test_feats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NaiveBayesClassifier example usage\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "nb = NaiveBayesClassifier.train(train_feats)\n",
    "nb.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classify unseen instance\n",
    "feat = bag_of_words(['the', 'plot', 'was', 'fantastic'])\n",
    "nb.classify(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate accuracy of classifier\n",
    "from nltk.classify.util import accuracy\n",
    "accuracy(nb, test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get classification probabilities for an instance\n",
    "probs = nb.prob_classify(test_feats[0][0]) # test_feats[0] is a tuple: (feats, label)\n",
    "probs.samples()  # classifier labels\n",
    "probs.prob('neg')  # probability of neg for this instance\n",
    "probs.max()  # max probability for this instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Classifier most informative words\n",
    "More informative features are those that occur primarily in one label and not on the other. \n",
    "The less informative features are those that occur frequently with both labels\n",
    "\"\"\"\n",
    "nb.most_informative_features(n=5)  # output format -> [(feature, value), (), (), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier most informative words along with probabilities for each label\n",
    "nb.show_most_informative_features(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NaiveBayesClassifier manual training\n",
    "from nltk.probability import DictionaryProbDist\n",
    "\n",
    "label_probdist = DictionaryProbDist({'pos': 0.5, 'neg': 0.5})\n",
    "true_probdist = DictionaryProbDist({True: 1})\n",
    "feature_probdist = {('pos', 'yes'):true_probdist, ('neg', 'no'):true_probdist}\n",
    "classifier = NaiveBayesClassifier(label_probdist, feature_probdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manual NaiveBayesClassifier example usage\n",
    "classifier.classify({'yes': True})  # output -> 'pos'\n",
    "classifier.classify({'no': True})  # output -> 'neg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Training a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "# support_cuttoff: The minimum number of instances that are required to make a decision about a feature.\n",
    "# binary=True because all features are binary\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "dt = DecisionTreeClassifier.train(train_feats, binary=True, \n",
    "                                  entropy_cutoff=0.8, depth_cutoff=5, support_cutoff=30)\n",
    "accuracy(dt, test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree calculate entropy manualy\n",
    "# remember : most informative decision has less entropy\n",
    "from nltk.probability import FreqDist, MLEProbDist, entropy\n",
    "fd = FreqDist({'pos':30, 'neg':10})\n",
    "entropy(MLEProbDist(fd))  # output -> 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Training scikit-learn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SkLearn Classifier version 1\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB # Choose and import an sklearn algorithm\n",
    "\n",
    "sk_classifier = SklearnClassifier(MultinomialNB()) # Construct an SklearnClassifier class with the chosen algorithm\n",
    "sk_classifier.train(train_feats) # Train the SklearnClassifier class with your training features\n",
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sklearn Classifier version 2\n",
    "# Bernoulli discret classifier based on NaiveBayes, bernoulli is binary 0 or 1\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "sk_classifier = SklearnClassifier(BernoulliNB())\n",
    "sk_classifier.train(train_feats)\n",
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sklearn Classifier version 3\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sk_classifier = SklearnClassifier(LogisticRegression())\n",
    "sk_classifier.train(train_feats)\n",
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Classifier version 1\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "sk_classifier = SklearnClassifier(SVC())\n",
    "sk_classifier.train(train_feats)\n",
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Classifier version 2\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "sk_classifier = SklearnClassifier(LinearSVC())\n",
    "sk_classifier.train(train_feats)\n",
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Classifier version 3\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "\n",
    "sk_classifier = SklearnClassifier(NuSVC())\n",
    "sk_classifier.train(train_feats)\n",
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5 Measuring precision and recall of a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# precision recall\n",
    "import collections\n",
    "from nltk.metrics import precision, recall\n",
    "\n",
    "def precision_recall(classifier, testfeats): # the same arguments you pass to accuracy()\n",
    "    refsets = collections.defaultdict(set)  # real output\n",
    "    testsets = collections.defaultdict(set)  # system output \n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "    for label in classifier.labels():\n",
    "        precisions[label] = precision(refsets[label], testsets[label]) # len(reference.intersection(test)) / len(test)\n",
    "        recalls[label] = recall(refsets[label], testsets[label])# len(reference.intersection(test)) / len(reference)\n",
    "    return precisions, recalls  # output format -> {'label1': float, 'label2': float, 'label3':float, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# precision recall example usage\n",
    "nb_precisions, nb_recalls = precision_recall(nb, test_feats)\n",
    "nb_precisions['pos']\n",
    "nb_recalls['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6 Considering high information words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A high information word is a word that is strongly biased towards a single classification label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.collections import defaultdict\n",
    "\n",
    "# labeled_words must be this format -> [('label', ['w', 'w', ...]), (), (),...]\n",
    "def high_information_words(labeled_words, score_fn=BigramAssocMeasures.chi_sq, min_score=5):\n",
    "    word_fd = FreqDist()\n",
    "    label_word_fd = ConditionalFreqDist()\n",
    "    for label, words in labeled_words:\n",
    "        for word in words:\n",
    "            word_fd[word] += 1\n",
    "            label_word_fd[label][word] += 1\n",
    "    n_xx = label_word_fd.N() # the total frequency for all words in all labels\n",
    "    high_info_words = set()\n",
    "    for label in label_word_fd.conditions():\n",
    "        n_xi = label_word_fd[label].N() # the total frequency of all words that occurred for the label\n",
    "        word_scores = defaultdict(int)\n",
    "        for word, n_ii in label_word_fd[label].items():\n",
    "            n_ix = word_fd[word] # the total frequency of the word across all labels\n",
    "            score = score_fn(n_ii, (n_ix, n_xi), n_xx)\n",
    "            word_scores[word] = score\n",
    "        bestwords = [word for word, score in word_scores.items() if score >= min_score]\n",
    "        high_info_words |= set(bestwords)  # union of two sets\n",
    "    return high_info_words # output format -> {'w', 'w', 'w', ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# high information words example usage\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "labels = movie_reviews.categories()\n",
    "labeled_words = [(l, movie_reviews.words(categories=[l])) for l in labels]\n",
    "high_info_words = high_information_words(labeled_words)\n",
    "\n",
    "feat_det = lambda words: bag_of_words_in_set(words, high_info_words)  # custom feature detector\n",
    "lfeats_with = label_feats_from_corpus(movie_reviews, feature_detector=feat_det)\n",
    "train_feats_with, test_feats_with = split_label_feats(lfeats_with)\n",
    "\n",
    "lfeats_without = label_feats_from_corpus(movie_reviews)\n",
    "train_feats_without, test_feats_without = split_label_feats(lfeats_without)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Classifiers WITH high information words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NaiveBayes\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "nb_classifier = NaiveBayesClassifier.train(train_feats_with)\n",
    "accuracy(nb_classifier, test_feats_with)\n",
    "nb_precisions_with, nb_recalls_with = precision_recall(nb_classifier, test_feats_with)\n",
    "nb_precisions_with['pos']\n",
    "nb_recalls_with['neg']\n",
    "\n",
    "# Maxent\n",
    "from nltk.classify import MaxentClassifier\n",
    "me_classifier = MaxentClassifier.train(train_feats_with, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "accuracy(me_classifier, test_feats_with)\n",
    "me_precisions_with, me_recalls_with = precision_recall(me_classifier, test_feats_with)\n",
    "me_precisions_with['pos']\n",
    "me_recalls_with['neg']\n",
    "\n",
    "# DecisionTree\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier.train(train_feats_with, binary=True, depth_cutoff=20, \n",
    "                                             support_cutoff=20, entropy_cutoff=0.01)\n",
    "accuracy(dt_classifier, test_feats_with)\n",
    "\n",
    "# Sklearn\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "sk_classifier = SklearnClassifier(LinearSVC()).train(train_feats_with)\n",
    "accuracy(sk_classifier, test_feats_with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Classifiers WITHOUT high information words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NaiveBayes\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "nb_classifier = NaiveBayesClassifier.train(train_feats_without)\n",
    "accuracy(nb_classifier, test_feats_without)\n",
    "nb_precisions_without, nb_recalls_without = precision_recall(nb_classifier, test_feats_without)\n",
    "nb_precisions_without['pos']\n",
    "nb_recalls_without['neg']\n",
    "\n",
    "# Maxent\n",
    "from nltk.classify import MaxentClassifier\n",
    "me_classifier = MaxentClassifier.train(train_feats_without, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "accuracy(me_classifier, test_feats_without)\n",
    "me_precisions_without, me_recalls_without = precision_recall(me_classifier, test_feats_without)\n",
    "me_precisions_without['pos']\n",
    "me_recalls_without['neg']\n",
    "\n",
    "# DecisionTree\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier.train(train_feats_without, binary=True, depth_cutoff=5, \n",
    "                                             support_cutoff=30, entropy_cutoff=0.8)\n",
    "accuracy(dt_classifier, test_feats_without)\n",
    "\n",
    "# Sklearn\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "sk_classifier = SklearnClassifier(LinearSVC()).train(train_feats_without)\n",
    "accuracy(sk_classifier, test_feats_without)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7 Combining classifiers with voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.probability import FreqDist\n",
    "class MaxVoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        self._labels = sorted(set(itertools.chain(*[c.labels() for c in classifiers])))\n",
    "    def labels(self):  # required to implement\n",
    "        return self._labels\n",
    "    def classify(self, feats):  # required to implement\n",
    "        counts = FreqDist()\n",
    "        for classifier in self._classifiers:\n",
    "            counts[classifier.classify(feats)] += 1\n",
    "        return counts.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "itertools.chain: Make an iterator that returns elements from the first iterable until it is exhausted,\n",
    "then proceeds to the next iterable, until all of the iterables are exhausted.\n",
    "Used for treating consecutive sequences as a single sequence\n",
    "\"\"\"\n",
    "import itertools\n",
    "a = itertools.chain('ab', 'cd', [1,2])\n",
    "for i in a:\n",
    "    print(i, end=' ')  # output -> a b c d 1 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = itertools.chain(['ab', 'cd', [1,2]])\n",
    "for i in a:\n",
    "    print(i, end=' ')  # output -> ab cd [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = itertools.chain(*['ab', 'cd', [1,2]])  # uses *args\n",
    "for i in a:\n",
    "    print(i, end=' ')  # output -> a b c d 1 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MaxVoteClassifier example usage\n",
    "mv_classifier = MaxVoteClassifier(nb_classifier, dt_classifier, me_classifier, sk_classifier)\n",
    "accuracy(mv_classifier, test_feats_with)\n",
    "mv_precisions, mv_recalls = precision_recall(mv_classifier, test_feats_with)\n",
    "mv_precisions['pos']\n",
    "mv_recalls['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.8 Multi-label classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The reuters corpus contains multi-labeled text\n",
    "# multi label classifier\n",
    "# multi_label_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "def reuters_high_info_words(score_fn=BigramAssocMeasures.chi_sq):\n",
    "    labeled_words = []\n",
    "    for label in reuters.categories():\n",
    "        labeled_words.append((label, reuters.words(categories=[label])))\n",
    "    return high_information_words(labeled_words, score_fn=score_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract train and test features from reuters multilabel corpus \n",
    "def reuters_train_test_feats(feature_detector=bag_of_words): # we will be overriding this using bag_of_words_in_set()\n",
    "    train_feats = []\n",
    "    test_feats = []\n",
    "    for fileid in reuters.fileids():\n",
    "        feats = feature_detector(reuters.words(fileid))\n",
    "        labels = reuters.categories(fileid)\n",
    "        if fileid.startswith('training'):\n",
    "            featlist = train_feats  # featlist is a reference variable\n",
    "        else: # fileid.startswith('test')\n",
    "            featlist = test_feats\n",
    "        featlist.append((feats, labels))\n",
    "    return train_feats, test_feats  # format of output -> [(featureset, [label])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rwords = reuters_high_info_words()\n",
    "featdet = lambda words: bag_of_words_in_set(words, rwords)\n",
    "multi_train_feats, multi_test_feats = reuters_train_test_feats(featdet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train binary clssifiers to use in multilabeledClassifer\n",
    "# trainf is training function\n",
    "# labelled_feats is a list of multi-label feature sets\n",
    "# labelset is a set of possible labels\n",
    "def train_binary_classifiers(trainf, labelled_feats, labelset):\n",
    "    pos_feats = collections.defaultdict(list)\n",
    "    neg_feats = collections.defaultdict(list)\n",
    "    classifiers = {}\n",
    "    for feat, labels in labelled_feats:\n",
    "        for label in labels:\n",
    "            pos_feats[label].append(feat)\n",
    "        for label in labelset - set(labels):\n",
    "            neg_feats[label].append(feat)\n",
    "    for label in labelset:\n",
    "        postrain = [(feat, label) for feat in pos_feats[label]]\n",
    "        negtrain = [(feat, '!%s' % label) for feat in neg_feats[label]]\n",
    "        classifiers[label] = trainf(postrain + negtrain)\n",
    "    return classifiers  # binary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_binary_classifiers example usage\n",
    "trainf = lambda train_feats: SklearnClassifier(LogisticRegression()).train(train_feats)\n",
    "labelset = set(reuters.categories())\n",
    "classifiers = train_binary_classifiers(trainf, multi_train_feats, labelset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import MultiClassifierI\n",
    "\n",
    "class MultiBinaryClassifier(MultiClassifierI):\n",
    "    def __init__(self, *label_classifiers): # takes a list of labeled classifiers of the form [(label, classifier)]\n",
    "        self._label_classifiers = dict(label_classifiers)\n",
    "        self._labels = sorted(self._label_classifiers.keys())\n",
    "    def labels(self):  # required to implement\n",
    "        return self._labels \n",
    "    def classify(self, feats):  # required to implement\n",
    "        lbls = set()\n",
    "        for label, classifier in self._label_classifiers.items():\n",
    "            if classifier.classify(feats) == label:\n",
    "                lbls.add(label)\n",
    "        return lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multi_classifier = MultiBinaryClassifier(*classifiers.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To evaluate this classifier, we can use precision and recall, but not accuracy.\n",
    "That's because the accuracy function assumes single values, and doesn't take into account partial matches. \n",
    "For example, if the multi_classifier returns three labels for a feature set, \n",
    "and two of them are correct but the third is not, then the accuracy() function would mark that as incorrect. \n",
    "So, instead of using accuracy, we will use masi distance, which measures the partial overlap between two sets. \n",
    "If the masi distance is close to 0, the better the match. But if the masi distance is close to 1, there is little or no overlap\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# precision, recall and average masi distance for MultiBinaryClassifier\n",
    "from nltk.metrics import masi_distance, precision, recall\n",
    "# multi_classifier is a MultiBinaryClassifier\n",
    "# test_feats is a multi-label feature set\n",
    "def multi_metrics(multi_classifier, test_feats):\n",
    "    mds = []  # holds masi distance \n",
    "    refsets = collections.defaultdict(set)  # real output\n",
    "    testsets = collections.defaultdict(set)  # system output\n",
    "    for i, (feat, labels) in enumerate(test_feats):\n",
    "        for label in labels:\n",
    "            refsets[label].add(i)\n",
    "        guessed = multi_classifier.classify(feat)\n",
    "        for label in guessed:\n",
    "            testsets[label].add(i)\n",
    "        mds.append(masi_distance(set(labels), guessed))\n",
    "    avg_md = sum(mds) / len(mds)\n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "    for label in multi_classifier.labels():\n",
    "        precisions[label] = precision(refsets[label], testsets[label])\n",
    "        recalls[label] = recall(refsets[label], testsets[label])\n",
    "    return precisions, recalls, avg_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate MultiBinaryClassifier\n",
    "multi_precisions, multi_recalls, avg_md = multi_metrics(multi_classifier, multi_test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(reuters.fileids(categories=['soybean']))\n",
    "multi_precisions['soybean']\n",
    "multi_recalls['soybean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "note : In general, the labels that have more feature sets will have higher precision and recall, \n",
    "and those with less feature sets will have lower performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Epub File</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Article Spinner</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Unsupervised learning\n",
    "Article Spinning: Taking an existing article that’s very popular, \n",
    "and modifying certain words or phrases so that it doesn’t exactly match the original, \n",
    "which then prevents the search engines from marking it as duplicate content\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How? Replace words with their synonyms\n",
    "# We will use the trigram for this. We’ll use the previous word and the next word to predict the current word:\n",
    "# P(w(i) | w(i-1), w(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries been used \n",
    "import nltk\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup # Python library for parsing XML and HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract positive reviews from Amazon reviews\n",
    "positive_review = BeautifulSoup(open('sorted_data_acl/electronics/positive.review').read(), 'lxml')\n",
    "positive_review = positive_review.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will use trigrams to generate new content\n",
    "trigram = {}  # output format -> {(context):[middleWord1, md2, md3, ...]}\n",
    "for review in positive_review:\n",
    "    s = review.text.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    for i in range(len(tokens)-2):\n",
    "        k = (tokens[i], tokens[i+2])  # context\n",
    "        if k not in trigram: # a better way is to use collections.defaultdict\n",
    "            trigram[k] = []\n",
    "        trigram[k].append(tokens[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probability vector\n",
    "from collections import defaultdict\n",
    "\n",
    "# format -> {(context): {'w': P}}\n",
    "trigram_probabilities = {} # the values are dictionary from word->probability\n",
    "\n",
    "for context, center in trigram.items():\n",
    "    if len(set(center)) > 1: # only do this when there are different options for a middle word\n",
    "        d = defaultdict(float)\n",
    "        for word in center:\n",
    "            d[word] += 1\n",
    "        n = len(center)\n",
    "        for word in d:\n",
    "            d[word] /= n\n",
    "        trigram_probabilities[context] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate random sample\n",
    "import random\n",
    "\n",
    "def random_sample(d):\n",
    "    r = random.random()\n",
    "    cum = 0\n",
    "    \n",
    "    for w,p in d.items():\n",
    "        cum += p\n",
    "        if r < cum:\n",
    "            return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_spinner():\n",
    "    review = random.choice(positive_review)\n",
    "    s = review.text.lower()\n",
    "    print('original: %s' %s)\n",
    "    \n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    for i in range(len(tokens)-2):\n",
    "        if random.random()<0.2: # 20% chance of replacement\n",
    "            k = (tokens[i], tokens[i+2])\n",
    "            if k in trigram_probabilities:\n",
    "                w = random_sample(trigram_probabilities[k])\n",
    "                tokens[i+1] = w\n",
    "    print(\" \".join(tokens)\n",
    "          .replace(\" .\", \".\").replace(\" '\", \"'\").replace(\" ,\", \",\").replace(\"$ \", \"$\").replace(\" !\", \"!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Article Spinner usage\n",
    "test_spinner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
