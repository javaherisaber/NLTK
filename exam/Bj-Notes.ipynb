{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#download - nltk download\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sent_tokenizer\n",
    "\n",
    "#کلمات مخفف- هم مکان ها و کلماتی که در شروع جمله ها میایند یک مدل میسازه. unsupervised - روی متن بزرگ\n",
    "# it's instance of \"PunktSentenceTokenizer\" . \"\"everytime train model\"\"\n",
    "\n",
    "# A1\n",
    "from nltk.tokenize import sent_tokenize\n",
    "a=sent_tokenize('Hi students! Welcome to our course.')\n",
    "\n",
    "#A2\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "tokenizer.tokenize('Hi students! Welcome to our course.')\n",
    "\n",
    "# #hazm #sent_tokenize\n",
    "from hazm import sent_tokenize\n",
    "sent_tokenize('سلام چطوری؟ اسم تو بگو توپولی')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #word_tokenizer #word_tokenize\n",
    "\n",
    "\n",
    "para = \"he can't do it!\"\n",
    "\n",
    "# A1 - TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "treebank = word_tokenize(para)\n",
    "\n",
    "# A2 - TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "treebank=tokenizer.tokenize(para)\n",
    "\n",
    "#### تری بنک یک مدل از پیش آماده نیست . و بر اساس مدل استخراج شده از یک کورپس به همین اسم و با استفاده از\n",
    "#### رجکس ساخته شده\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### #WordPunktTokenizer\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "wordPunkt=tokenizer.tokenize(\"can't is a contraction.\")\n",
    "\n",
    "\n",
    "\n",
    "#### #hazm #word_tokenize\n",
    "from hazm import word_tokenize\n",
    "\n",
    "hazmWordTokenize = word_tokenize('سلام سلام چطوری اسم تو بگو توپولی')\n",
    "\n",
    "treebank,wordPunkt,hazmWordTokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #sent_tokenize #Tokenizing #sentences #regex #regular #word_tokenize \n",
    "\n",
    "# برای زمانی که بخواهیم کنترل کامل روی توکن کردن متن داشته باشیم.\n",
    "\n",
    "# RegexTokenizer => TokenizerI  --> why use \"RegexTokenizer\" instead of \"re.findall()\" ?\n",
    "# use another pars of nltk package - for exp : corpus reader oprtional tokenizer \n",
    "# must be an instance of TokenizerI\n",
    "\n",
    "\n",
    "# A1\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "t = RegexpTokenizer(r\"[\\w']+\")\n",
    "t.tokenize(\"can't is a contraction.\")\n",
    "\n",
    "# A2\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "regexp_tokenize(\"can't is a contraction.\", \"[\\w']+\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #sent_tokenize #sentence #tokenize #train\n",
    "\n",
    "# PunktSentenceTokenizer از طریق یادگیری بدون نظارت یادمیگیره چجوری جملات رو بشنکنه\n",
    "# مثلا اگر یک توکن معمولا با نقطه بیاید احتمالا خلاصه است.\n",
    "# mr.amini\n",
    "\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "\n",
    "pre_trained = sent_tokenize(text)\n",
    "our_trained = sent_tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "# A2\n",
    "\n",
    "with open('~/nltk_data/corpora/webtext/overheard.txt',encoding='ISO-8859-2') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "sent_tokenizer = PunktSentenceTokenizer(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_trained[678],our_trained[678]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stopword\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "stopwords.fileids() # for #corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wordnet #synsets\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "syns = wn.synsets('book') #list of synset\n",
    "\n",
    "wn.synset('book.n.01') == syns[0]\n",
    "\n",
    "syns[0].name(),syns[0].definition(),syns[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wordnet #hypernyms = فراشمول یا انتزاعی #hyponyms = زیرشمول\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "syn = wn.synset('book.n.01')\n",
    "\n",
    "syn.hypernyms()[0].hyponyms()\n",
    "\n",
    "syn.root_hypernyms() # ریشه درخت - انتزاعی ترین حالت \n",
    "\n",
    "syn.hypernym_paths() # لیستی از لیست ها - مسیر تا ریشه که ممکن از چند مسیر باشد\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "#similarity #shortest_path #common\n",
    "\n",
    "s1 = wn.synsets('car')[0]\n",
    "s2 = wn.synsets('computer')[0]\n",
    "\n",
    "##########  شباهت یابی - راه 1\n",
    "\n",
    "#اگر هایپرنیم های مشترک نداشته باشند نمیتوان با استفاده ازین محاسبه کرد  و  نان برمیگرذاند\n",
    "\n",
    "s1.wup_similarity(s2) # یک عدد بین 0 و ۱ هرچی بیشتر شبیه تر\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########راه 2 - هر چی فاصله بیشتر کم تر شبیه اند\n",
    "\n",
    "#هایپرنیم های دو سینست را که یکسان اند تا ریشه بدست می آورد\n",
    "#s1.hypernym_paths() and s2....\n",
    "\n",
    "s1.common_hypernyms(s2)\n",
    "\n",
    "\n",
    "\n",
    "########## راه 3\n",
    "\n",
    "s1.path_similarity(s2)\n",
    "\n",
    "\n",
    "########## راه 4\n",
    "\n",
    "#leacock chordorow\n",
    "\n",
    "s1.lch_similarity(s2)  ## هر چی بیشتر شبیه تر -- محدوده ای براشون پیدا نکردم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordnet #part of speech #pos \n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "syn = wn.synset('book.n.01')\n",
    "\n",
    "syn.pos() #notice - pos just for \"synset\" not synsets\n",
    "\n",
    "# -----------------------------------\n",
    "\n",
    "syns = wn.synsets('book',pos='n') # just nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wordnet #lemma #antonyms\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "s = wn.synset('cookbook.n.01')\n",
    "\n",
    "s.lemmas() #all lemma\n",
    "\n",
    "s.lemmas()[0].name(),s.lemmas()[0].synset()\n",
    "\n",
    "# ------------------------------------------\n",
    "\n",
    "\n",
    "s = wn.synset('good.a.01')\n",
    "s.lemmas()[0].antonyms()[0]  # \"some\" lemma has antonyms : return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#collocations #bigram #bigramcollocationsfinder\n",
    "\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "words = [word.lower() for word in webtext.words('grail.txt')]\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "bcf.apply_word_filter(lambda w:len(w)<3 or w in stopwords.words('english'))\n",
    "\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio,4)\n",
    "\n",
    "\n",
    "#trigram #trigramcollocations\n",
    "\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "words = [word.lower() for word in webtext.words('singles.txt')]\n",
    "tcf = TrigramCollocationFinder.from_words(words)\n",
    "\n",
    "tcf.apply_freq_filter(3) #باید سه بار یا بیشتر تکرار شده باشند تا کولوکیشن حساب شوند \n",
    "tcf.apply_word_filter(lambda x:len(x)<3 or x in stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "tcf.nbest(TrigramAssocMeasures.likelihood_ratio,4) #score_fn,n    n top scores\n",
    "\n",
    "tcf.score_ngrams(TrigramAssocMeasures.likelihood_ratio)  #score all ngrams\n",
    "\n",
    "tcf.above_score(TrigramAssocMeasures.likelihood_ratio,5) #score_fn,min_score   all ngrams with greater min_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'میرو'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming #stemmer #lancaster #regexp #snowball\n",
    "\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer,RegexpStemmer,SnowballStemmer\n",
    "\n",
    "#porter\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('cookery')\n",
    "\n",
    "#lancaster \n",
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('cookery')\n",
    "\n",
    "#regexp\n",
    "stemmer = RegexpStemmer('ing')\n",
    "stemmer.stem('booking')\n",
    "\n",
    "#snowball\n",
    "SnowballStemmer.languages\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmer.stem('booking')\n",
    "\n",
    "#hazm\n",
    "from hazm import Stemmer\n",
    "stemmer = Stemmer()\n",
    "stemmer.stem('میرویم')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cooking', 'cooking', 'cook')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "l = WordNetLemmatizer()\n",
    "#default pos = noun\n",
    "l.lemmatize('cooking'),l.lemmatize('cooking',pos='n'),l.lemmatize('cooking',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replacement pattern #contractions\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "rp = [\n",
    "    ('(\\w+)\\'ll','\\g<1> will'),\n",
    "    ('([i,I])\\'m','\\g<1> am')\n",
    "]\n",
    "\n",
    "rp = [(re.compile(regex),rep) for (regex,rep) in rp]\n",
    "\n",
    "s_in = \"i'm I'll\"\n",
    "\n",
    "for (pattern,rep) in rp:\n",
    "    s_in = re.sub(pattern,rep,s_in)\n",
    "\n",
    "word_tokenize(s_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looove to love #removing repeating character #backreference #backrefrence\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "class RepeatReplacer():\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.rep = r'\\1\\2\\3'\n",
    "    def replace(self,word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.pattern.sub(self.rep,word)\n",
    "        if word!=repl_word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "a=RepeatReplacer()\n",
    "a.replace('looove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pyenchant #enchant #spell correcting #spell correction\n",
    "\n",
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "class SpellingReplacer:\n",
    "    def __init__(self,dict_name='en',max_distance=2):\n",
    "        self.dict = enchant.Dict(dict_name)\n",
    "        self.max_distance = max_distance\n",
    "        \n",
    "    def replace(self,word):\n",
    "        if self.dict.check(word):\n",
    "            return word\n",
    "        else:\n",
    "            sugs = self.dict.suggest(word)\n",
    "            if sugs and edit_distance(word,sugs[0])<=self.max_distance:\n",
    "                return sugs[0]\n",
    "            else:\n",
    "                return word\n",
    "\n",
    "#note :\n",
    "\n",
    "#dict = enchant.Dict(dict_name)\n",
    "#dict.check(word) => True/False\n",
    "#dict.suggest(word) => list of suggestion\n",
    "\n",
    "#dict = enchant.DictWithPWL('en_US', 'my_words.txt')  ##personal word list\n",
    "\n",
    "enchant.list_dicts(),enchant.list_languages(),enchant.dict_exists('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replacing synonyms #replace synonyms #yaml #csv\n",
    "\n",
    "import csv\n",
    "import yaml\n",
    "\n",
    "class WordReplacer(object):\n",
    "    def __init__(self, word_map):\n",
    "        self.word_map = word_map\n",
    "    def replace(self, word):\n",
    "        return self.word_map.get(word, word)\n",
    "\n",
    "\n",
    "class CsvWordReplacer(WordReplacer):\n",
    "    def __init__(self, fname):\n",
    "        word_map = {}\n",
    "        for line in csv.reader(open(fname)):\n",
    "            word, syn = line\n",
    "            word_map[word] = syn\n",
    "        super().__init__(word_map)\n",
    "        \n",
    "\n",
    "class YamlWordReplacer(WordReplacer):\n",
    "    def __init__(self, fname):\n",
    "        word_map = yaml.load(open(fname))\n",
    "        super().__init__(word_map)\n",
    "        \n",
    "## yaml file example:=>  \n",
    "## name : xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#negations #Replacing negations with antonyms\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class AntonymReplacer(object):\n",
    "    def replace(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "        for syn in wordnet.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "        if len(antonyms) == 1: # unambiguous replacement\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def replace_negations(self, orig_sent):\n",
    "        sent = word_tokenize(orig_sent)\n",
    "        i, l = 0, len(sent)\n",
    "        words = []\n",
    "        while i < l:\n",
    "            word = sent[i]\n",
    "            if word == 'not' and i+1 < l:\n",
    "                ant = self.replace(sent[i+1])\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "        return words\n",
    "\n",
    "class AntonymWordReplacer(WordReplacer, AntonymReplacer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#POS tagging #tag #tagger #untagging #unigram #bigram #trigram #quadgram\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "sents = treebank.sents() \n",
    "\n",
    "# all tags : http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "tagged_sents = treebank.tagged_sents() # [ [('Pierre', 'NNP'),('Vinken', 'NNP'),...],... ]\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "#default tagger\n",
    "\n",
    "# چون تمامی این تگر ها از کلاس پدر یکسانی ارث بری میکنند توابع زیر بینشان یکسان است\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "\n",
    "tagger = DefaultTagger('NN')\n",
    "\n",
    "tagger.tag(sents[0]) #one sentence\n",
    "tagger.tag_sents(sents) #list of sentences\n",
    "tagger.tag_sents([['you','are','beautiful'],['shame','on','you']])\n",
    "\n",
    "test_sents = tagged_sents[3000:]\n",
    "tagger.evaluate(test_sents)  ## تگر آموزش دیده را بهش دیتا لیبل دار دادیم تا تست کنه خودشو\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "#UnigramTagger(train=None, model=None,backoff=None, cutoff=0, verbose=False)\n",
    "\n",
    "#cutoff : موارد زیر این تعداد تکرار را در نظر نمیگیرد و با آن ها مدل نمیسازد\n",
    "#model : prepared model -> model={'Pierre': 'NN'}\n",
    "#backoff : اگر تگر نتونست تگ بزنه میره سراغ این یکی\n",
    "\n",
    "from nltk.tag import UnigramTagger\n",
    "\n",
    "tagger = UnigramTagger(train = tagged_sents[:3000])\n",
    "\n",
    "#tagger.tag(sents[0])\n",
    "#tagger.tag(sents)\n",
    "\n",
    "tagger.evaluate(tagged_sents[3000:])\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "#NgramTagger(n, train=None, model=None,backoff=None, cutoff=0, verbose=False)\n",
    "\n",
    "from nltk.tag import NgramTagger\n",
    "\n",
    "class QuadgramTagger(NgramTagger):\n",
    "    def __init__(self,train=None, model=None,backoff=None, cutoff=0, verbose=False):\n",
    "        NgramTagger.__init__(self,4,train,model,backoff,cutoff,verbose)\n",
    "\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "#backoff tagging\n",
    "\n",
    "from nltk.tag import UnigramTagger,BigramTagger,TrigramTagger\n",
    "\n",
    "# نکته : اول تریگرام هارو میزنه بعد بایگرام ها بعد ...\n",
    "def backoff_tagger(train_sents,tagger_classes,backoff=None):\n",
    "    for tagger_class in tagger_classes:\n",
    "        backoff = tagger_class(train=train_sents,backoff = backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "tagger=backoff_tagger(tagged_sents[:3000],[UnigramTagger,BigramTagger,TrigramTagger,QuadgramTagger],DefaultTagger('NN'))\n",
    "\n",
    "tagger._taggers #[<TrigramTagger: size=845>,<BigramTagger: size=1859>,<UnigramTagger: size=8818>,<DefaultTagger: tag=NN>]\n",
    "\n",
    "tagger.evaluate(tagged_sents[3000:])\n",
    "\n",
    "tagger._context_to_tag \n",
    "\n",
    "\n",
    "#---------------------------------------------------\n",
    "#save tags #store tags #save model\n",
    "\n",
    "#import pickle\n",
    "# f = open('tagger.pickle', 'wb')\n",
    "# pickle.dump(tagger, f)\n",
    "# f.close()\n",
    "# f = open('tagger.pickle', 'rb')\n",
    "# tagger = pickle.load(f)\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "#Untagging a tagged sentence\n",
    "\n",
    "from nltk.tag import untag\n",
    "\n",
    "untag([('Hello', 'NN'), ('students', 'NN')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FreqDist #ConditionalFreqDist #?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9632203755665876"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create custome model with FreqDist and Conditional FreqDist\n",
    "\n",
    "#ساخت یک مدل از ۱۰۰۰ کلمه ای که تقریبا همیشه یک تگ یکسان دارند.\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "from nltk.probability import FreqDist,ConditionalFreqDist\n",
    "\n",
    "\n",
    "def word_tag_model(words,tagged_words,limit=1000):\n",
    "    fd = FreqDist(words)\n",
    "    cfd = ConditionalFreqDist(tagged_words)\n",
    "    \n",
    "    # cfd['book'] ==> FreqDist({'NN': 7, 'VB': 1})\n",
    "    # fd.most_common(2) ==> [(',', 4885), ('the', 4045)]\n",
    "    \n",
    "    most_common = [word for word,count in fd.most_common()]\n",
    "    return {word:cfd[word].max() for word in most_common}\n",
    "\n",
    "model=word_tag_model(treebank.words(),treebank.tagged_words())\n",
    "\n",
    "tagger=backoff_tagger(tagged_sents[:3000],[UnigramTagger,BigramTagger,TrigramTagger,QuadgramTagger],DefaultTagger('NN'))\n",
    "\n",
    "custom_tagger = UnigramTagger(model=model,backoff=tagger)\n",
    "\n",
    "custom_tagger.evaluate(tagged_sents[3000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16861644722641916"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tagging with regex #regex\n",
    "\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "#این ها به ترتیب از بالا به پایین چک می شوند. الگوری اخری برای زمانی است که هیچ کدام از \n",
    "# بالایی ها چک نشده باشند.\n",
    "\n",
    "patterns = [\n",
    "    (r'^\\d+$','CD'),\n",
    "    (r'.*ing$','VBG'),\n",
    "    (r'.*ful$', 'JJ'),\n",
    "    (r'.*','NN')\n",
    "]\n",
    "\n",
    "tagger = RegexpTagger(patterns)\n",
    "tagger.evaluate(tagged_sents[3000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#affix tagger #prefix #suffix\n",
    "\n",
    "from nltk.tag import AffixTagger\n",
    "\n",
    "#AffixTagger(train=None, model=None, affix_length=-3,min_stem_length=2, backoff=None, cutoff=0, verbose=False)\n",
    "\n",
    "#min_stem_length =>>>  |affix_length| + min_stem_length <= len(word)  else==> return None\n",
    "#affix_length =>>> if pos-> word[:affix_length], if neg ->  word[affix_length:]\n",
    "\n",
    "# اگر -2 بود دو کاراکتر اخر و اگر +2 بود دو کاراکتر اول را یاد میگیرد.\n",
    "\n",
    "\n",
    "# order for backoff => first check : -3 -> -2 -> 2 -> 3\n",
    "\n",
    "\n",
    "#--------------------------------------------------------\n",
    "\n",
    "tagger = AffixTagger(train=tagged_sents[:3000],min_stem_length=0,affix_length=0)  # equal to UnigramTagger\n",
    "\n",
    "tagger.evaluate(tagged_sents[3000:])\n",
    "\n",
    "#tagger._context_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Brill Tagger #brill trainer  #?\n",
    "\n",
    "\n",
    "from nltk.tag import brill,brill_trainer\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "tagged_sents = treebank.tagged_sents()\n",
    "\n",
    "templates = [\n",
    "    brill.Template(brill.Word([1,2]),brill.Word([-1])), # words\n",
    "    brill.Template(brill.Pos([1,2]),brill.Pos([-1]))\n",
    "]\n",
    "\n",
    "trainer = brill_trainer.BrillTaggerTrainer(DefaultTagger('NN'),templates,deterministic=True)\n",
    "tagger = trainer.train(tagged_sents[:3000])\n",
    "\n",
    "tagger.evaluate(tagged_sents[3000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.892467083962875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tnt #tnt tagger #second order Markov models\n",
    "\n",
    "from nltk.tag import tnt\n",
    "from nltk.tag import DefaultTagger\n",
    "\n",
    "# tnt.TnT(unk=None, Trained=False, N=1000, C=False) \n",
    "\n",
    "#unk -> tagger for unkown words\n",
    "#if unk is pretrained Trained=True else => False  ---> notice : for all sequentialBackoffTagger set True.\n",
    "#C : True if you want capitalization of words to be significant\n",
    "\n",
    "tagger=tnt.TnT(DefaultTagger('NN'),Trained=True)\n",
    "tagger.train(tagged_sents[:3000])\n",
    "tagger.evaluate(tagged_sents[3000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wordnet #tagging with wordnet #wordnet tagging\n",
    "\n",
    "#v -> VB   verb\n",
    "# n -> NN  noun\n",
    "# a -> JJ  adjective\n",
    "# s -> JJ  adjective\n",
    "# r -> RB  adverb\n",
    "\n",
    "\n",
    "\n",
    "#دقت کن . فقط دو تااز توابع رو ارث بری کرده و برای کار کافی است.\n",
    "# به تنهایی نتیجه ی خوبی ندارد ولی به صورت بک آف برای یونیگرام و بایگرام و... خیلی مناسب است.\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import SequentialBackoffTagger\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "class WordNetTagger(SequentialBackoffTagger):\n",
    "    '''\n",
    "     >>> wt = WordNetTagger()\n",
    "     >>> wt.tag(['food', 'is', 'great'])\n",
    "     [('food', 'NN'), ('is', 'VB'), ('great', 'JJ')]\n",
    "     '''\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        SequentialBackoffTagger.__init__(self, *args, **kwargs)\n",
    "        self.wordnet_tag_map = {\n",
    "            'n': 'NN', \n",
    "            's': 'JJ', \n",
    "            'a': 'JJ', \n",
    "            'r': 'RB', \n",
    "            'v': 'VB' \n",
    "        }\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        word = tokens[index]\n",
    "        fd = FreqDist()\n",
    "        for synset in wordnet.synsets(word):\n",
    "            fd[synset.pos()] += 1\n",
    "        if len(fd)==0:\n",
    "            return 'NN'\n",
    "        return self.wordnet_tag_map.get(fd.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'NNP'), ('book', None)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tagging proper names #SequentialBackoffTagger\n",
    "\n",
    "from nltk.corpus import names\n",
    "from nltk.tag import SequentialBackoffTagger\n",
    "\n",
    "class NamesTagger(SequentialBackoffTagger):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        SequentialBackoffTagger.__init__(self,*args,**kwargs)\n",
    "        self._name_set = set([w.lower() for w in names.words()])\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        if tokens[index].lower() in self._name_set:\n",
    "            return 'NNP'\n",
    "        return None\n",
    "    \n",
    "nt = NamesTagger()\n",
    "nt.tag(['James', 'book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ClassifierBasedTagger #most_accurate_tagger #most accurate # معمولا بهترین نتیجه را دارد"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size=\"8\" color=\"red\">معمولا بهترین تگر هست</font>\n",
    "<br><br><br>\n",
    "<font size=\"4\">The ClassifierBasedPOSTagger class is a subclass of ClassifierBasedTagger that implements a feature detector. The feature detector finds multiple length suffixes, does some regular expression matching, and looks at the unigram, bigram, and trigram history to produce a fairly complete set of features for each word.\n",
    "<br>\n",
    "It only implements a feature_detector() method. All the training and tagging is done in ClassifierBasedTagger. It defaults to training a NaiveBayesClassifier class. To use other classifiers, pass in your own classifier_builder function:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ClassifierBasedTagger #most_accurate_tagger #most accurate # معمولا بهترین نتیجه را دارد\n",
    "\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.tag import ClassifierBasedPOSTagger\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "train_sents = treebank.tagged_sents()\n",
    "\n",
    "me_tagger = ClassifierBasedPOSTagger(train=train_sents[:3000],classifier_builder=MaxentClassifier.train)\n",
    "me_tagger.evaluate(train_sents)\n",
    "\n",
    "# custome feature detector \n",
    "\n",
    "from nltk.tag.sequential import ClassifierBasedTagger\n",
    "\n",
    "def unigram_feature_detector(tokens, index, history):\n",
    "    return {'word':tokens[index]} # shuld return a dictionary of feature-name:feature-value\n",
    "\n",
    "tagger = ClassifierBasedTagger(train=train_sents, feature_detector=unigram_feature_detector)\n",
    "tagger.evaluate(test_sents)\n",
    "\n",
    "\n",
    "\n",
    "# with backoff  :  cutoff_prob اگر این رو نزاریم\n",
    "# استفاده از بک آف فایده ای نداره چون این همیشه بهترین رو برمیگردونه\n",
    "default = DefaultTagger('NN')\n",
    "tagger = ClassifierBasedPOSTagger(train=train_sents, cutoff_prob=0.3, backoff=default)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a tagger with NLTK-Trainer\n",
    "# انتخاب بهترین روش و پارامتر ها توسط کد نوشته شده زیر:\n",
    "\n",
    "https://github.com/japerk/nltk-trainer\n",
    "\n",
    "python train_tagger.py treebank --fraction 0.75 --no-pickle \n",
    "\n",
    "75% for train, other for test.\n",
    "AffixTagger + UnigramTagger + BigramTagger + TrigramTagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bag of words\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "def bag_of_words(words:list)->dict:\n",
    "    return {word:True for word in words}\n",
    "\n",
    "def bag_of_words_not_in_set(words,badwords):\n",
    "    return bag_of_words(set(words)-set(badwords))\n",
    "\n",
    "def bag_of_bigrams_words(words,score_fn=BigramAssocMeasures.chi_sq,n=200):\n",
    "    bcf = BigramCollocationFinder.from_words(words)\n",
    "    bigrams=bcf.nbest(score_fn,n)\n",
    "    return bag_of_words(bigrams+words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#naive bayes #training naive bayes classifier\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "from collections import defaultdict\n",
    "\n",
    "# {'label1':[ {'hello':True,'see':True},...] , 'label2':...}\n",
    "def label_feats_from_corpus(corp, feature_detector=bag_of_words):\n",
    "    label_feats = defaultdict(list)\n",
    "    for cat in corp.categories():\n",
    "        for fileid in corp.fileids(categories = [cat]):\n",
    "            featureset = feature_detector(corp.words(fileids=[fileid]))\n",
    "            label_feats[cat].append(featureset)\n",
    "    return label_feats\n",
    "\n",
    "\n",
    "# [(feat,label),...]  ---> [({'hello':True},'label1'),...]\n",
    "def split_label_feats(lfeats,split=0.75):\n",
    "    test_feats = list()\n",
    "    train_feats = list()\n",
    "    for label,feats in lfeats.items():\n",
    "        cutoff = int(len(feats)*split)\n",
    "        train_feats.extend([(feat,label) for feat in feats[:cutoff]])\n",
    "        test_feats.extend([(feat,label) for feat in feats[cutoff:]])\n",
    "    return train_feats,test_feats\n",
    "\n",
    "label_feats = label_feats_from_corpus(movie_reviews)\n",
    "\n",
    "train,test = split_label_feats(label_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.728"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#naive bayes #training naive bayes classifier\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "nb = NaiveBayesClassifier.train(train)\n",
    "\n",
    "\n",
    "\n",
    "## for classification a sample :\n",
    "\n",
    "feat = bag_of_words(['the', 'plot', 'was', 'fantastic'])\n",
    "\n",
    "# فقط بهترین لیبل را خروجی میدهد\n",
    "nb.classify(feat)\n",
    "\n",
    "# احتمال هر لیبل را محاسبه میکند و خروجی میدهد\n",
    "probs = nb.prob_classify(feat)\n",
    "\n",
    "probs.samples() #dict_keys(['neg', 'pos'])\n",
    "\n",
    "probs.prob('neg'),probs.prob('pos')\n",
    "\n",
    "probs.max() # best choice for label => 'pos'\n",
    "\n",
    "nb.most_informative_features(n=5)\n",
    "\n",
    "nb.show_most_informative_features(n=5)\n",
    "\n",
    "#----------------------------------------------\n",
    "\n",
    "## accuracy(trained_classifier, test_feats)\n",
    "\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "accuracy(nb,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decision Tree #DecisionTree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>entropy_cutoff</b>: If the entropy of the probability distribution of label choices in the tree is greater than the entropy_cutoff value, then the tree is refined further by creating more branches. <b>Higher values of entropy_cutoff will decrease both accuracy and training time.</b>\n",
    "\n",
    "<b>depth_cutoff</b>: The default value is 100, which means that classification may require up to 100 decisions before reaching a leaf node.\n",
    "\n",
    "<b>support_cuttoff</b>: The minimum number of instances that are required to make a decision about a feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier.train(train, binary=True, \n",
    "                                  entropy_cutoff=0.8, depth_cutoff=5, support_cutoff=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scikitlearn #scikit learn #classifier \n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "skc = SklearnClassifier(MultinomialNB())\n",
    "skc.train(train)\n",
    "accuracy(skc,test) # 0.83\n",
    "\n",
    "\n",
    "### OTHER ::: دقیقا شبیه بالایی اجرا می شوند\n",
    "\n",
    "# from sklearn.naive_bayes import BernoulliNB : 0.812\n",
    "# ==>can also work with discrete values by binarizing those values, so that the final values are 1 or 0.\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression : 0.892\n",
    "\n",
    "# from sklearn.svm import SVC : 0.69\n",
    "# ==>Support vector machine -> خود ان ال تی کی توی خودش اینو نداره\n",
    "\n",
    "# from sklearn.svm import LinearSVC : 0.864\n",
    "\n",
    "# from sklearn.svm import NuSVC : 0.882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#precision #recall\n",
    "\n",
    "import collections\n",
    "from nltk.metrics import precision,recall\n",
    "\n",
    "def precision_recall(classifier,testfeats):\n",
    "    refsets = defaultdict(set)\n",
    "    testsets = defaultdict(set)\n",
    "    for i,(feats,label) in enumerate(testfeats):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "    precision,recall = {},{}\n",
    "    for label in classifier.labels():\n",
    "        precision[label] = precision(refsets[label],testsets[label])\n",
    "        recall[label] = recall(refsets[label],testsets[label])\n",
    "    return precision,recall\n",
    "\n",
    "# the result for naive bayes:\n",
    "\n",
    "# nb_precisions['pos'], nb_recalls['pos'] == (0.651595744680851, 0.98)\n",
    "# nb_precisions['neg'], nb_recalls['neg'] == (0.9596774193548387, 0.476)\n",
    "\n",
    "## The conclusion could be that there are certain common words that are biased towards the pos label, \n",
    "## but occur frequently enough in the neg feature sets to cause mis-classifications.\n",
    "\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "## USE  \"from sklearn.svm import NuSVC\"\n",
    "\n",
    "# SklearnClassifier class of NuSVC weighs its features according to its own internal model. \n",
    "# Words that are more significant are those that occur primarily in a single label, and will get higher weights in the model.\n",
    "# Words that are common to both labels will get lower weights, as they are less significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#high information words #show_most_informative_features() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <font color='green'><b>high information word</b></font> is a word that is <font color='red'><b>strongly biased towards a single classification label</b></font>.\n",
    "\n",
    "Use show_most_informative_features() method on both the NaiveBayesClassifier class and the MaxentClassifier class. The top words are <font color='green'><b>different for both classifiers</b></font> due to how each classifier calculates the significance of each feature, and it's actually beneficial to have these different methods as <font color='green'><b>they can be combined to improve accuracy</b></font> (next recipe).\n",
    "\n",
    "<br><br>\n",
    "<font size=\"4\">Eliminating low informative words from the training data can actually improve accuracy, precision, and recall.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#high information words #show_most_informative_features() \n",
    "\n",
    "# score_fn(n_ii, (n_ix, n_xi), n_xx) : ارزش کلمه به ازای یک لیبل خاص را بررسی میکند\n",
    "# n_ii : مجموع تکرار کلمه در لیبل خاص\n",
    "# n_ix : مجموع تکرار کلمه در کل لیبل ها\n",
    "# n_xi : مجموع کل کلمات یک لیبل\n",
    "# n_xx : مجموع کل کلمات برای تمامی لیبل ها\n",
    "\n",
    "from nltk.probability import FreqDist,ConditionalFreqDist\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import collections\n",
    "\n",
    "def high_information_words(labeled_words, score_fn=BigramAssocMeasures.chi_sq, min_score=5):\n",
    "    word_fd = FreqDist()\n",
    "    label_word_fd = ConditionalFreqDist()\n",
    "    for label, words in labeled_words:\n",
    "        for word in words:\n",
    "            word_fd[word] += 1\n",
    "            label_word_fd[label][word] += 1\n",
    "    n_xx = label_word_fd.N() # the total frequency for all words in all labels\n",
    "    high_info_words = set()\n",
    "    for label in label_word_fd.conditions():\n",
    "        n_xi = label_word_fd[label].N() # the total frequency of all words that occurred for the label\n",
    "        word_scores = collections.defaultdict(int)\n",
    "        for word, n_ii in label_word_fd[label].items():\n",
    "            n_ix = word_fd[word] # the total frequency of the word across all labels\n",
    "            score = score_fn(n_ii, (n_ix, n_xi), n_xx)\n",
    "            word_scores[word] = score\n",
    "        bestwords = [word for word, score in word_scores.items() if score >= min_score]\n",
    "        high_info_words |= set(bestwords)\n",
    "    return high_info_words\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "labels = movie_reviews.categories()\n",
    "\n",
    "labeled_words = [(l,movie_reviews.words(categories=l)) for l in labels]\n",
    "\n",
    "high_words = high_information_words(labeled_words)\n",
    "\n",
    "lfeat = label_feats_from_corpus(movie_reviews,lambda words: bag_of_words(set(words) & set(high_words)))\n",
    "\n",
    "train_feats_with,test_feats_with = split_label_feats(lfeat)\n",
    "\n",
    "## SAMPLE CODE:\n",
    "\n",
    "# nb_classifier = NaiveBayesClassifier.train(train_feats_with) : with high informative words\n",
    "# accuracy(nb_classifier, test_feats_with)\n",
    "# nb_precisions_with['pos'], nb_recalls_with['pos']\n",
    "# nb_precisions_with['neg'], nb_recalls_with['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>(precision,recall)\n",
    "\n",
    "<b><font color=\"green\">NaiveBayesClassifier</font></b>\n",
    "<br>\n",
    "<b>without</b>: 0.728\n",
    "<br> pos : (0.651595744680851, 0.98)\n",
    "<br> neg : (0.9596774193548387, 0.476)\n",
    "<br><br>\n",
    "<b>with</b>: 0.91\n",
    "<br> pos : (0.8988326848249028, 0.924)\n",
    "<br> neg : (0.9218106995884774, 0.896)\n",
    "<br><br>\n",
    "<b><font color=\"green\">MaxentClassifier</font></b>\n",
    "<br>\n",
    "<b>without</b>: 0.722\n",
    "<br> pos : (0.6456692913385826, 0.984)\n",
    "<br> neg : (0.9663865546218487, 0.46)\n",
    "<br><br>\n",
    "<b>with</b>: 0.912\n",
    "<br> pos : (0.8992248062015504, 0.928)\n",
    "<br> neg : (0.9256198347107438, 0.896)\n",
    "<br><br>\n",
    "<b><font color=\"green\">DecisionTreeClassifier</font></b>\n",
    "<br>\n",
    "<b>without</b>: 0.68\n",
    "<br>\n",
    "<b>with</b>: 0.706\n",
    "<br><br>\n",
    "<b><font color=\"green\">SklearnClassifier</font></b>\n",
    "<br>\n",
    "<b>without</b>: 0.864\n",
    "<br>\n",
    "<b>with</b>: 0.86\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "nb_classifier = NaiveBayesClassifier.train(train_feats_with)\n",
    "# accuracy(nb_classifier, test_feats_with)\n",
    "\n",
    "from nltk.classify import MaxentClassifier\n",
    "me_classifier = MaxentClassifier.train(train_feats_with, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier.train(train_feats_with, binary=True, depth_cutoff=5, \n",
    "                                             support_cutoff=30, entropy_cutoff=0.8)\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "sk_classifier = SklearnClassifier(LinearSVC()).train(train_feats_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#voting #vote #combining classifier with voting\n",
    "\n",
    "\n",
    "import itertools\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.probability import FreqDist\n",
    "class MaxVoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        self._labels =sorted(set(itertools.chain(*[c.labels() for c in classifiers])))\n",
    "    def labels():\n",
    "        return self._labels\n",
    "    def classify(self,feats):\n",
    "        counts = FreqDist()\n",
    "        for classifier in self._classifiers:\n",
    "            counts[classifier.classify(feats)] +=1\n",
    "        return counts.max()\n",
    "\n",
    "        \n",
    "# itertools.chain:\n",
    "# [c.labels() for c in classifiers] == [['neg', 'pos'], ['pos', 'neg'],...]\n",
    "# list(itertools.chain([c.labels() for c in classifiers])) == [['neg', 'pos'], ['pos', 'neg'],...]\n",
    "# list(itertools.chain(*[c.labels() for c in classifiers])) == ['neg', 'pos','neg', 'pos',...]\n",
    "# list(itertools.chain(*[c.labels() for c in classifiers])) == {'neg','pos'}\n",
    "\n",
    "\n",
    "# itertools.chain('ab', 'cd', [1,2]) == a b c d 1 2 \n",
    "# itertools.chain(['ab', 'cd', [1,2]]) == ab cd [1, 2]\n",
    "# itertools.chain(*['ab', 'cd', [1,2]]) == a b c d 1 2\n",
    "\n",
    "\n",
    "mv_classifier = MaxVoteClassifier(nb_classifier, dt_classifier, me_classifier, sk_classifier)\n",
    "mv_classifier._labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#multilabel #multi label #multilabel classifier\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# we need this format : (label, words)\n",
    "def reuters_high_info_words(score_fn=BigramAssocMeasures.chi_sq):\n",
    "    labels = reuters.categories()\n",
    "    labeled_words = list()\n",
    "    for label in labels:\n",
    "        labeled_words.append((label,reuters.words(categories=label)))\n",
    "    return high_information_words(labeled_words,score_fn)\n",
    "\n",
    "#create test_feats and train_feats  format: (feats,labels)\n",
    "def reuters_train_test_feats(feature_detector=bag_of_words):\n",
    "    train_feats = list()\n",
    "    test_feats = list()\n",
    "    for fileid in reuters.fileids():\n",
    "        feats = feature_detector(reuters.words(fileids=[fileid]))\n",
    "        labels = reuters.categories(fileids=[fileid])\n",
    "        if fileid.startswith('training'):\n",
    "            train_feats.append((feats,labels))\n",
    "        else:\n",
    "            test_feats.append((feats,labels))\n",
    "    return train_feats,test_feats\n",
    "            \n",
    "#get the words\n",
    "hiws = reuters_high_info_words()\n",
    "multi_train_feats,multi_test_feats = reuters_train_test_feats(lambda words:bag_of_words(set(hiws) & set(words)))\n",
    "\n",
    "\n",
    "#train binary classification\n",
    "def train_binary_classifiers(trainf, labelled_feats, labelset):\n",
    "    pos = defaultdict(list)\n",
    "    neg = defaultdict(list)\n",
    "    classifiers = dict()\n",
    "    \n",
    "    for feats,labels in labelled_feats:\n",
    "        for label in labels:\n",
    "            pos[label].append((feats,label))\n",
    "        for label in set(labelset)-set(labels):\n",
    "            neg[label].append((feats,'!'+label))\n",
    "    for label in labelset:\n",
    "        classifiers[label] = trainf(pos[label]+neg[label])\n",
    "    return classifiers\n",
    "\n",
    "\n",
    "\n",
    "trainf = lambda train_feats:SklearnClassifier(LogisticRegression()).train(train_feats)\n",
    "classifiers=train_binary_classifiers(trainf,multi_train_feats,reuters.categories())\n",
    "len(classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#multilabeled classifier\n",
    "\n",
    "from nltk.classify import MultiClassifierI\n",
    "\n",
    "class MultiBinaryClassifier(MultiClassifierI):\n",
    "    def __init__(self,*classifiers):\n",
    "        self._classifiers = dict(classifiers)\n",
    "        self._labels = self._classifiers.keys()\n",
    "    def labels():\n",
    "        return self._labels\n",
    "    def classify(self,feats):\n",
    "        labels = set()\n",
    "        for label,classifier in self._classifiers.items():\n",
    "            if classifier.classify(feats)==label:\n",
    "                labels.add(label)\n",
    "        return labels\n",
    "\n",
    "\n",
    "multi_classifier = MultiBinaryClassifier(*classifiers.items())\n",
    "\n",
    "from nltk.metrics import masi_distance, precision, recall\n",
    "def multi_metrics(multi_classifier, test_feats):\n",
    "    mds = []\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "    for i, (feat, labels) in enumerate(test_feats):\n",
    "        for label in labels:\n",
    "            refsets[label].add(i)\n",
    "        guessed = multi_classifier.classify(feat)\n",
    "        for label in guessed:\n",
    "            testsets[label].add(i)\n",
    "        mds.append(masi_distance(set(labels), guessed))\n",
    "    avg_md = sum(mds) / len(mds)\n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "    for label in multi_classifier.labels():\n",
    "        precisions[label] = precision(refsets[label], testsets[label])\n",
    "        recalls[label] = recall(refsets[label], testsets[label])\n",
    "    return precisions, recalls, avg_md\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "multi_precisions, multi_recalls, avg_md = multi_metrics(multi_classifier, multi_test_feats)\n",
    "\n",
    "multi_precisions['soybean'], multi_recalls['soybean'], len(reuters.fileids(categories=['soybean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Spinner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: \n",
      "this is truly a no-brainer, excellent headphones at a great price, wonderful all around phones, would make a great gift also\n",
      "\n",
      "this is truly a no-brainer, excellent headphones at a great price, wonderful all the phones, would make a great gift also\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "positive_review = BeautifulSoup(open('/home/aminbag/nltk_data/sorted_data_acl/electronics/positive.review'),'lxml')\n",
    "positive_review = positive_review.findAll('review_text')\n",
    "positive_review = [review.get_text() for review in positive_review]\n",
    "\n",
    "\n",
    "# create trigrams : { (W1,W3):[W2,W2-1,...],... }\n",
    "trigrams = defaultdict(list)\n",
    "for review in positive_review:\n",
    "    review = review.lower()\n",
    "    words = nltk.tokenize.word_tokenize(review)\n",
    "    for i in range(len(words)-2):\n",
    "        trigrams[(words[i],words[i+2])].append(words[i+1])\n",
    "\n",
    "\n",
    "# محاسبه احتمال مربوط به هر یک از مقادیر\n",
    "trigram_probabilities = {} # the values are dictionary from word->probability\n",
    "\n",
    "for contex,center in trigrams.items():\n",
    "    if len(set(center))>1:\n",
    "        d = defaultdict(float)\n",
    "        for word in center:\n",
    "            d[word]+=1\n",
    "        n = len(center)\n",
    "        for word in d.keys():\n",
    "            d[word] /= n\n",
    "            \n",
    "        trigram_probabilities[contex] = d\n",
    "\n",
    "\n",
    "# انتخاب یک رندوم بر اساس وزن های کلمات\n",
    "import random\n",
    "def random_sample(d:dict):\n",
    "    r = random.random()\n",
    "    cum = 0\n",
    "    \n",
    "    for word,prob in d.items():\n",
    "        cum +=prob\n",
    "        if r<cum:\n",
    "            return word\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def test_spinner():\n",
    "    review = random.choice(positive_review)\n",
    "    s = review.lower()\n",
    "    print('original: %s' %s)\n",
    "    \n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    for i in range(len(tokens)-2):\n",
    "        if random.random()<0.2: # 20% chance of replacement\n",
    "            k = (tokens[i], tokens[i+2])\n",
    "            if k in trigram_probabilities:\n",
    "                w = random_sample(trigram_probabilities[k])\n",
    "                tokens[i+1] = w\n",
    "    print(\" \".join(tokens)\n",
    "          .replace(\" .\", \".\").replace(\" '\", \"'\").replace(\" ,\", \",\").replace(\"$ \", \"$\").replace(\" !\", \"!\"))\n",
    "    \n",
    "\n",
    "\n",
    "test_spinner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b', 'google.com')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string = 'purple b@google.com amin@gmail.com monkey dishwasher'\n",
    "match = re.findall(r'(\\w+)@([\\w.-]+)', string)\n",
    "if match:\n",
    "    print(match[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
